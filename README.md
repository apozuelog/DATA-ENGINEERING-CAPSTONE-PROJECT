![logo](img/logo.png)  


# A Data Engineer project.  

**What is Ford GoBike?**
Ford GoBike is the Bay Area's bike share system. Bay Area Bike Share was introduced in 2013 as a pilot program for the region, with 700 bikes and 70 stations across San Francisco and San Jose. Once expansion is complete, Ford GoBike will grow to 7,000 bikes across San Francisco, the East Bay and San Jose.  

<img src="img/mapa.PNG" alt="Maph" style="width: 400px;"/>

Ford GoBike, like other bike share systems, consists of a fleet of specially designed, sturdy and durable bikes that are locked into a network of docking stations throughout the city. The bikes can be unlocked from one station and returned to any other station in the system, making them ideal for one-way trips. People use bike share to commute to work or school, run errands, get to appointments or social engagements and more. It's a fun, convenient and affordable way to get around.

![maph](img/bici.jpg)

The bikes are available for use 24 hours/day, 7 days/week, 365 days/year and riders have access to all bikes in the network when they become a member or purchase a pass.


### Scope the Project and Gather Data  

The objective of optimizing the number of bicycles in each station and studying the behavior of the GoBike user.
After the exploratory analysis of the data, I have chosen to create a DataWarehouse in Redshift for storage and exploitation of the data using BI tools such as Quicksight or Tableau. The ETL will be developed with Airflow to automate the upload through task.
In principle, a large number of users is not expected for the exploitation of the data.

Steps:
1. Raw storage of files (in our case CSV) in AWS S3.
2. Exploratory analysis and data wrangling with Spark
3. Definition of the data model
4. Create dimension tables and fact tables in Redshift
5. Development in Airflow of the necessary operators to load data in our Redshift model  

<img src="img/aws_achitecture.PNG" alt="GoBike architecture"/>


#### 1. Raw storage of files in AWS S3
In our S3 we have a series of csv with the information generated by the shared GoBike bikes.
Total size: 558.4 MB in 16 objects.


#### 2. Exploratory analysis with PySpark
Using the Jupyter Noetbook **bikes_SparkSession.ipynb**, I perform the necessary tasks to carry out the exploratory analysis of the data, cleaning and developing the script to export the results to Parquet.
After a first read of all the CSV files hosted on AWS S3, we get a dataset with more than 2.7 million records in 16 columns and includes records from 2018-01-01 to 2019-09-30.  
**STAGING TRIPS**  
![schema](img/staging_schema.PNG)  


#### 3. Definition of the data model
Examining the schema I determine that the best architecture for the exploitation of these is to develop a BB.DD. star with dimension tables and fact tables. In this way we can answer the following questions:   
* The times of the journeys
* From where and where with geolocation
* Age, gender and type of user  

In addition columns of year, week, month, day and hour have been added for a better subsequent analysis  

**Bike Trips star data model**  
<img src="img/model.PNG" alt="GoBike star model" style="width:400px;"/>  


#### 4. Create dimension tables and fact tables in Redshift
In the file **create_tables.sql** there are all the queries necessary to create the tables in Redshift.
The result is as follows:
**DIM TABLES**  
<table class="">
  <tr>
    <td><img src="img/time_table_schema.PNG" alt="GoBike star model" style="width:400px;"/></td>
    <td><img src="img/station_table_schema.PNG" alt="GoBike star model" style="width:400px;"/></td>
  </tr>
  <tr>
    <td><img src="img/user_table_schema.PNG" alt="GoBike star model" style="width:400px;"/></td>
    <td><img src="img/bikes_table_schema.PNG" alt="GoBike star model" style="width:400px;"/></td>
  </tr>
</table>

**FACT TABLE**  
![schema](img/bike_trips_table_schema.PNG)


#### 5. Development in Airflow of the necessary operators to load data in our Redshift model 
**OPERATORS**  
* **StageToRedshiftOperator**
* **LoadDimensionOperator**
* **LoadFactOperator**
* **DataQualityOperator**  
![schema](img/graph_execution.PNG)  

**Charging times**  
![schema](img/gant_execution.PNG)  

**All task OK!!**  
![schema](img/tree_view.PNG)  
