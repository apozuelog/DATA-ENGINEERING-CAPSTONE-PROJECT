![logo](img/logo.png)  


# A Data Engineer project.  

**What is Ford GoBike?**
Ford GoBike is the Bay Area's bike share system. Bay Area Bike Share was introduced in 2013 as a pilot program for the region, with 700 bikes and 70 stations across San Francisco and San Jose. Once expansion is complete, Ford GoBike will grow to 7,000 bikes across San Francisco, the East Bay and San Jose.  

<img src="img/mapa.PNG" alt="Maph" style="width: 400px;"/>

Ford GoBike, like other bike share systems, consists of a fleet of specially designed, sturdy and durable bikes that are locked into a network of docking stations throughout the city. The bikes can be unlocked from one station and returned to any other station in the system, making them ideal for one-way trips. People use bike share to commute to work or school, run errands, get to appointments or social engagements and more. It's a fun, convenient and affordable way to get around.

![maph](img/bici.jpg)

The bikes are available for use 24 hours/day, 7 days/week, 365 days/year and riders have access to all bikes in the network when they become a member or purchase a pass.


### Scope the Project and Gather Data  

The objective of optimizing the number of bicycles in each station and studying the behavior of the GoBike user.
After the exploratory analysis of the data, I have chosen to create a DataWarehouse in Redshift for storage and exploitation of the data using BI tools such as Quicksight or Tableau. The ETL will be developed with Airflow to automate the upload through task.
In principle, a large number of users is not expected for the exploitation of the data.

Steps:
1. Raw storage of files (in our case CSV) in AWS S3.
2. Exploratory analysis and data wrangling with Spark
3. Definition of the data model
4. Create dimension tables and fact tables in Redshift
5. Development in Airflow of the necessary operators to load data in our Redshift model  
6. Forecast. HDFS  

<img src="img/aws_achitecture.PNG" alt="GoBike architecture"/>


#### 1. Raw storage of files in AWS S3
In our S3 we have a series of csv with the information generated by the shared GoBike bikes.
Total size: 558.4 MB in 16 objects.


#### 2. Exploratory analysis with PySpark
Using the Jupyter Noetbook **bikes_SparkSession.ipynb**, I perform the necessary tasks to carry out the exploratory analysis of the data, cleaning and developing the script to export the results to Parquet.
After a first read of all the CSV files hosted on AWS S3, we get a dataset with more than 2.7 million records in 16 columns and includes records from 2018-01-01 to 2019-09-30.  
**STAGING TRIPS**  
![schema](img/staging_schema.PNG)  


#### 3. Definition of the data model
Examining the schema I determine that the best architecture for the exploitation of these is to develop a BB.DD. star with dimension tables and fact tables. In this way we can answer the following questions:   
* The times of the journeys
* From where and where with geolocation
* Age, gender and type of user  

In addition columns of year, week, month, day and hour have been added for a better subsequent analysis  

**Bike Trips star data model**  
<img src="img/model.PNG" alt="GoBike star model" style="width:400px;"/>  


#### 4. Create dimension tables and fact tables in Redshift
In the file **create_tables.sql** there are all the queries necessary to create the tables in Redshift.
The result is as follows:
**DIM TABLES**  
<table class="">
  <tr>
    <td><img src="img/time_table_schema.PNG" alt="GoBike star model" style="width:400px;"/></td>
    <td><img src="img/station_table_schema.PNG" alt="GoBike star model" style="width:400px;"/></td>
  </tr>
  <tr>
    <td><img src="img/user_table_schema.PNG" alt="GoBike star model" style="width:400px;"/></td>
    <td><img src="img/bikes_table_schema.PNG" alt="GoBike star model" style="width:400px;"/></td>
  </tr>
</table>

**FACT TABLE**  
![schema](img/bike_trips_table_schema.PNG)


#### 5. Development in Airflow of the necessary operators to load data in our Redshift model 
**OPERATORS**  
* **StageToRedshiftOperator**
* **LoadDimensionOperator**
* **LoadFactOperator**
* **DataQualityOperator**  
![schema](img/graph_execution.PNG)  

**Charging times**  
![schema](img/gant_execution.PNG)  

**All task OK!!**  
![schema](img/tree_view.PNG)  


#### 6. Forecast.  HDFS
To anticipate hyperpotential scenarios in which the volume of data doubles or triples, a change in the project architecture is planned.

**HDFS architecture**
The strengths of Hadoop architecture
Hadoop's architecture allows for effective analysis of unstructured big data, adding value that can help you make strategic decisions, improve production processes, save costs, track what your customers think, or draw conclusions scientific, say.

Its scalable technology makes it possible, its speed (not in real time, at least not without help, like the one provided by Spark), flexibility, among other strengths. If we have to point out its five main advantages, they would be the following:

Highly scalable technology: A Hadoop cluster can grow simply by adding new nodes. It is not necessary to make adjustments that modify the initial structure. Therefore, it allows us easy growth, without being tied to the initial characteristics of the design, making use of dozens of low-cost servers that, unlike the relational database, cannot scale. Thanks to MapReduce's distributed processing, files are easily divided into blocks.

Low-cost storage: Information is not stored by default, in rows and columns, as is the case with traditional databases, but Hadoop assigns categorized data through thousands of cheap computers, and this is a great saving. Only then does it become feasible. Otherwise, we would not be able to work with large volumes of data, since the cost would be extremely high, unaffordable for the vast majority of companies.

Flexibility: By increasing the number of system nodes we also gain in storage and processing capacity. In turn, it is possible to add or access new and different data sources (structured, semi-structured and unstructured), while there is the possibility of adapting accessory tools that work in the Hadoop environment and help in the design of processes, integration or improve other aspects.

Speed: Low cost, scalability and flexibility will be of little use to us if the result is not reasonably fast. Fortunately, Hadoop also allows for very fast processing and analysis.

Fault tolerant: Hadoop is a technology that makes it easy to store large volumes of information, which in turn allows you to retrieve data safely. If a computer goes down, another copy is always available, making data recovery possible in the event of a crash.

As a possible solution there is AWS EMR and Spark 2 using Scala.

**Since this is not the case at the moment, the solution provided is the best.**

